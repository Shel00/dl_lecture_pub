{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回演習課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNISTデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_x, mnist_y = mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\")\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_x, mnist_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1：Denoising auto-encoderの実装．また，MNISTを用いて次のことを確認．\n",
    "* reconstruction errorが小さくなっている（学習が進んでいる）．\n",
    "* 重みの可視化（特徴の可視化）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "theano_rng = RandomStreams(rng.randint(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(params,gparams,lr=0.1):\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam in zip(params, gparams):\n",
    "        updates[param] = param - lr * gparam\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoderクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    def __init__(self,visible_dim,hidden_dim,function):\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.function = function\n",
    "        \n",
    "        self.W = theano.shared(rng.uniform(\n",
    "                    low=-4*np.sqrt(6. / (visible_dim + hidden_dim)),\n",
    "                    high=4*np.sqrt(6. / (visible_dim + hidden_dim)),\n",
    "                    size=(visible_dim,hidden_dim)\n",
    "                ).astype(\"float32\"),name=\"W\")\n",
    "        \n",
    "        self.a = theano.shared(np.zeros(visible_dim).astype(np.float32),name=\"a\")\n",
    "        self.b = theano.shared(np.zeros(hidden_dim).astype(np.float32),name=\"b\")\n",
    "        self.params = [self.W,self.a,self.b]\n",
    "        \n",
    "    #encoder\n",
    "    def encode(self,x):\n",
    "        u = T.dot(x, self.W)+self.b\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "    \n",
    "    #decoder\n",
    "    def decode(self,x):\n",
    "        u = T.dot(x, self.W.T)+self.a\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "    \n",
    "    #forward propagation\n",
    "    def prop(self,x):\n",
    "        y = self.encode(x)\n",
    "        reconst_x = self.decode(y)\n",
    "        return reconst_x\n",
    "    \n",
    "    #reconstruction error\n",
    "    def reconst_error(self,x,noise):\n",
    "        tilde_x = x*noise\n",
    "        reconst_x = self.prop(tilde_x)\n",
    "        error = T.mean(T.sum(T.nnet.binary_crossentropy(reconst_x,x),axis=1))\n",
    "        return error, reconst_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(train_X.shape[1],500,T.nnet.sigmoid)\n",
    "\n",
    "x = T.matrix('x')\n",
    "noise = T.matrix('noise')\n",
    "\n",
    "cost,reconst_x = model.reconst_error(x,noise)\n",
    "params  = model.params\n",
    "gparams = T.grad(cost, params)\n",
    "updates = sgd(params,gparams) \n",
    "\n",
    "train = theano.function([x,noise], [cost,reconst_x], updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corruption_level = 0.3\n",
    "batch_size = 100\n",
    "nbatches = train_X.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_X = shuffle(train_X)\n",
    "    err_all=[]\n",
    "    for i in range(0,nbatches):\n",
    "        start = i * batch_size\n",
    "        end   = start + batch_size\n",
    "        \n",
    "        noise = rng.binomial(size=train_X[start:end].shape, n=1, p=1-corruption_level)\n",
    "        err,reconst_x = train(train_X[start:end],noise)\n",
    "        err_all.append(err)\n",
    "    print \"Epoch:%d, Error:%lf\" %(epoch, np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの可視化\n",
    "* corruption_levelを変更して違いを観測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = model.W.get_value(borrow=True).T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight[i].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2：RBMの実装．また，MNISTを用いて次のことを確認．\n",
    "* reconstruction errorが小さくなっている（学習が進んでいる）．\n",
    "* 重みの可視化（特徴の可視化）．\n",
    "* 文字の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "theano_rng = RandomStreams(rng.randint(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBMクラスの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self,visible_dim,hidden_dim,function,k):\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.function = function\n",
    "        self.k = k\n",
    "        \n",
    "        self.W = theano.shared(rng.uniform(\n",
    "                    low=-4*np.sqrt(6. / (visible_dim + hidden_dim)),\n",
    "                    high=4*np.sqrt(6. / (visible_dim + hidden_dim)),\n",
    "                    size=(visible_dim,hidden_dim)\n",
    "                ).astype(\"float32\"),name=\"W\")\n",
    "        self.a = theano.shared(np.zeros(visible_dim).astype(\"float32\"),name=\"a\")\n",
    "        self.b = theano.shared(np.zeros(hidden_dim).astype(\"float32\"),name=\"b\")\n",
    "        \n",
    "        self.params = [self.W,self.a,self.b]\n",
    "        \n",
    "    def propup(self,input):\n",
    "        y = T.dot(input,self.W) + self.b\n",
    "        output = self.function(y)\n",
    "        return output\n",
    "    \n",
    "    def propdown(self,input):\n",
    "        y = T.dot(input,self.W.T) + self.a\n",
    "        output = self.function(y)\n",
    "        return output\n",
    "\n",
    "    #p(h|v)\n",
    "    def ph_v(self,v):\n",
    "        h = self.propup(v)\n",
    "        h_sample = theano_rng.binomial(size=h.shape,n=1, p=h, dtype=\"float32\")\n",
    "        return h_sample,h\n",
    "    \n",
    "    #p(v|h)\n",
    "    def pv_h(self,h):\n",
    "        v = self.propdown(h)\n",
    "        v_sample = theano_rng.binomial(size=v.shape,n=1, p=v, dtype=\"float32\")\n",
    "        return v_sample,v\n",
    "    \n",
    "    #gibbs sampling(h→v→h)\n",
    "    def gibbs_hvh(self,h):\n",
    "        v_sample,mean_v = self.pv_h(h)\n",
    "        h_sample,mean_h = self.ph_v(v_sample)\n",
    "        return h_sample,mean_h\n",
    "        \n",
    "    #gibbs sampling(v→h→v)\n",
    "    def gibbs_vhv(self,v):\n",
    "        h_sample,mean_h = self.ph_v(v)\n",
    "        v_sample,mean_v = self.pv_h(h_sample)\n",
    "        return v_sample,mean_v\n",
    "    \n",
    "    #cost(free energy)\n",
    "    def free_energy(self,input):\n",
    "        y = T.dot(input,self.W) + self.b\n",
    "        return -T.dot(input,self.a) -T.sum(T.log(1 + T.exp(y)),axis=1)\n",
    "    \n",
    "    #reconstruction error\n",
    "    def reconst_error(self,v):\n",
    "        v_sample = v\n",
    "        for k in range(self.k):\n",
    "            v_sample,mean_v = model.gibbs_vhv(v_sample)\n",
    "            \n",
    "        cross_entropy = T.mean(T.sum(T.nnet.binary_crossentropy(mean_v,v),axis=1))\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.matrix(\"x\")\n",
    "persistent = T.matrix(\"persistent\")\n",
    "\n",
    "#K(the number of gibbs sampling)\n",
    "K = 1\n",
    "model = RBM(train_X.shape[1],500,T.nnet.sigmoid,K)\n",
    "\n",
    "#persistent RBM\n",
    "_x = persistent\n",
    "for k in range(K):\n",
    "    _x,_  = model.gibbs_vhv(_x)\n",
    "\n",
    "cost    = T.mean(model.free_energy(x)) -T.mean(model.free_energy(_x))\n",
    "params  = model.params\n",
    "gparams = T.grad(cost, params, consider_constant=[_x])\n",
    "updates = sgd(params, gparams) \n",
    "err     = model.reconst_error(x)\n",
    "\n",
    "train = theano.function([x,persistent], [cost,_x], updates=updates, allow_input_downcast=True)\n",
    "reconst = theano.function([x], err, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nbatches = train_X.shape[0] // batch_size\n",
    "\n",
    "persistent = train_X.copy()\n",
    "for epoch in range(10):\n",
    "    train_X,persistent = shuffle(train_X,persistent)\n",
    "    err_all=[]\n",
    "    for i in range(0,nbatches):\n",
    "        start = i * batch_size\n",
    "        end   = start + batch_size\n",
    "        \n",
    "        cost,_x = train(train_X[start:end],persistent[start:end])\n",
    "        persistent[start:end]=_x\n",
    "        err = reconst(train_X[start:end])\n",
    "        err_all.append(err)\n",
    "    print \"Epoch:%d, Error:%lf\" %(epoch, np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = model.W.get_value(borrow=True).T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight[i].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gibbs sampling\n",
    "v = T.matrix(\"v\")\n",
    "[sample_v,mean_v],updates = theano.scan(fn=model.gibbs_vhv,outputs_info=[v,None],n_steps=1000)\n",
    "sample = theano.function([v], [sample_v[-1],mean_v[-1]],updates=updates,allow_input_downcast=True)\n",
    "\n",
    "#seed\n",
    "test_X = shuffle(test_X)\n",
    "sample_v,mean_v = sample(test_X[0:100])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(mean_v[i].reshape((28,28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked auto-encoderの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_x, mnist_y = mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルを完成させて提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training:: layer:0, Epoch:0, Error:101.459069\n",
      "Pre-training:: layer:0, Epoch:1, Error:79.938382\n",
      "Pre-training:: layer:0, Epoch:2, Error:76.342781\n",
      "Pre-training:: layer:0, Epoch:3, Error:74.486785\n",
      "Pre-training:: layer:0, Epoch:4, Error:73.174462\n",
      "Pre-training:: layer:0, Epoch:5, Error:72.175131\n",
      "Pre-training:: layer:0, Epoch:6, Error:71.299969\n",
      "Pre-training:: layer:0, Epoch:7, Error:70.544710\n",
      "Pre-training:: layer:0, Epoch:8, Error:69.943691\n",
      "Pre-training:: layer:0, Epoch:9, Error:69.375086\n",
      "Pre-training:: layer:1, Epoch:0, Error:194.870496\n",
      "Pre-training:: layer:1, Epoch:1, Error:176.897497\n",
      "Pre-training:: layer:1, Epoch:2, Error:174.064697\n",
      "Pre-training:: layer:1, Epoch:3, Error:172.439328\n",
      "Pre-training:: layer:1, Epoch:4, Error:171.190407\n",
      "Pre-training:: layer:1, Epoch:5, Error:170.225750\n",
      "Pre-training:: layer:1, Epoch:6, Error:169.412120\n",
      "Pre-training:: layer:1, Epoch:7, Error:168.746566\n",
      "Pre-training:: layer:1, Epoch:8, Error:168.173964\n",
      "Pre-training:: layer:1, Epoch:9, Error:167.708773\n",
      "Pre-training:: layer:2, Epoch:0, Error:144.704886\n",
      "Pre-training:: layer:2, Epoch:1, Error:128.455915\n",
      "Pre-training:: layer:2, Epoch:2, Error:126.295980\n",
      "Pre-training:: layer:2, Epoch:3, Error:125.256923\n",
      "Pre-training:: layer:2, Epoch:4, Error:124.554813\n",
      "Pre-training:: layer:2, Epoch:5, Error:124.069322\n",
      "Pre-training:: layer:2, Epoch:6, Error:123.659178\n",
      "Pre-training:: layer:2, Epoch:7, Error:123.322536\n",
      "Pre-training:: layer:2, Epoch:8, Error:123.017790\n",
      "Pre-training:: layer:2, Epoch:9, Error:122.797504\n",
      "EPOCH:: 1, Validation cost: 0.257, Validation F1: 0.929\n",
      "EPOCH:: 2, Validation cost: 0.211, Validation F1: 0.939\n",
      "EPOCH:: 3, Validation cost: 0.186, Validation F1: 0.945\n",
      "EPOCH:: 4, Validation cost: 0.170, Validation F1: 0.950\n",
      "EPOCH:: 5, Validation cost: 0.158, Validation F1: 0.953\n",
      "EPOCH:: 6, Validation cost: 0.148, Validation F1: 0.955\n",
      "EPOCH:: 7, Validation cost: 0.140, Validation F1: 0.959\n",
      "EPOCH:: 8, Validation cost: 0.134, Validation F1: 0.961\n",
      "EPOCH:: 9, Validation cost: 0.128, Validation F1: 0.962\n",
      "EPOCH:: 10, Validation cost: 0.123, Validation F1: 0.964\n",
      "EPOCH:: 11, Validation cost: 0.119, Validation F1: 0.965\n",
      "EPOCH:: 12, Validation cost: 0.115, Validation F1: 0.966\n",
      "EPOCH:: 13, Validation cost: 0.114, Validation F1: 0.968\n",
      "EPOCH:: 14, Validation cost: 0.109, Validation F1: 0.968\n",
      "EPOCH:: 15, Validation cost: 0.106, Validation F1: 0.970\n",
      "EPOCH:: 16, Validation cost: 0.104, Validation F1: 0.970\n",
      "EPOCH:: 17, Validation cost: 0.101, Validation F1: 0.970\n",
      "EPOCH:: 18, Validation cost: 0.099, Validation F1: 0.970\n",
      "EPOCH:: 19, Validation cost: 0.097, Validation F1: 0.971\n",
      "EPOCH:: 20, Validation cost: 0.096, Validation F1: 0.971\n",
      "EPOCH:: 21, Validation cost: 0.094, Validation F1: 0.973\n",
      "EPOCH:: 22, Validation cost: 0.092, Validation F1: 0.971\n",
      "EPOCH:: 23, Validation cost: 0.091, Validation F1: 0.973\n",
      "EPOCH:: 24, Validation cost: 0.090, Validation F1: 0.973\n",
      "EPOCH:: 25, Validation cost: 0.088, Validation F1: 0.974\n",
      "EPOCH:: 26, Validation cost: 0.087, Validation F1: 0.973\n",
      "EPOCH:: 27, Validation cost: 0.086, Validation F1: 0.974\n",
      "EPOCH:: 28, Validation cost: 0.085, Validation F1: 0.975\n",
      "EPOCH:: 29, Validation cost: 0.083, Validation F1: 0.975\n",
      "EPOCH:: 30, Validation cost: 0.083, Validation F1: 0.975\n",
      "EPOCH:: 31, Validation cost: 0.082, Validation F1: 0.975\n",
      "EPOCH:: 32, Validation cost: 0.080, Validation F1: 0.977\n",
      "EPOCH:: 33, Validation cost: 0.082, Validation F1: 0.975\n",
      "EPOCH:: 34, Validation cost: 0.079, Validation F1: 0.977\n",
      "EPOCH:: 35, Validation cost: 0.078, Validation F1: 0.977\n",
      "EPOCH:: 36, Validation cost: 0.077, Validation F1: 0.977\n",
      "EPOCH:: 37, Validation cost: 0.076, Validation F1: 0.978\n",
      "EPOCH:: 38, Validation cost: 0.076, Validation F1: 0.978\n",
      "EPOCH:: 39, Validation cost: 0.076, Validation F1: 0.978\n",
      "EPOCH:: 40, Validation cost: 0.075, Validation F1: 0.978\n",
      "EPOCH:: 41, Validation cost: 0.074, Validation F1: 0.978\n",
      "EPOCH:: 42, Validation cost: 0.073, Validation F1: 0.979\n",
      "EPOCH:: 43, Validation cost: 0.073, Validation F1: 0.979\n",
      "EPOCH:: 44, Validation cost: 0.072, Validation F1: 0.979\n",
      "EPOCH:: 45, Validation cost: 0.072, Validation F1: 0.979\n",
      "EPOCH:: 46, Validation cost: 0.072, Validation F1: 0.978\n",
      "EPOCH:: 47, Validation cost: 0.071, Validation F1: 0.979\n",
      "EPOCH:: 48, Validation cost: 0.071, Validation F1: 0.980\n",
      "EPOCH:: 49, Validation cost: 0.070, Validation F1: 0.980\n",
      "EPOCH:: 50, Validation cost: 0.069, Validation F1: 0.980\n"
     ]
    }
   ],
   "source": [
    "#Autoencoder (or RBM)\n",
    "class Autoencoder:\n",
    "    def __init__(self,visible_dim,hidden_dim,W,function):\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = W\n",
    "        self.function = function\n",
    "         \n",
    "        self.a = theano.shared(np.zeros(visible_dim).astype(np.float64),name=\"a\")\n",
    "        self.b = theano.shared(np.zeros(hidden_dim).astype(np.float64),name=\"b\")\n",
    "        self.params = [self.W,self.a,self.b]\n",
    "        \n",
    "    #encoder\n",
    "    def encode(self,x):\n",
    "        u = T.dot(x, self.W)+self.b\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "    \n",
    "    #decoder\n",
    "    def decode(self,x):\n",
    "        u = T.dot(x, self.W.T)+self.a\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "    \n",
    "    #forward propagation\n",
    "    def prop(self,x):\n",
    "        y = self.encode(x)\n",
    "        reconst_x = self.decode(y)\n",
    "        return reconst_x\n",
    "    \n",
    "    #reconstruction error\n",
    "    def reconst_error(self,x,noise):\n",
    "        tilde_x = x*noise\n",
    "        reconst_x = self.prop(tilde_x)\n",
    "        error = T.mean(T.sum(T.nnet.binary_crossentropy(reconst_x,x),axis=1))\n",
    "        return error, reconst_x\n",
    "        \n",
    "#SGD\n",
    "def sgd(params,gparams,lr=0.1):\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam in zip(params, gparams):\n",
    "        updates[param] = param - lr * gparam\n",
    "    return updates\n",
    "    \n",
    "#Multi Layer Perceptron\n",
    "class Layer:\n",
    "    def __init__(self, in_dim, out_dim, function):\n",
    "        ## WRITE ME Done\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.function = function\n",
    "        self.W = theano.shared(\n",
    "                                rng.uniform(\n",
    "                                            low=-0.08, \n",
    "                                            high=0.08, \n",
    "                                            size=(in_dim, out_dim)\n",
    "                                ).astype(\"float64\"), name=\"W\")\n",
    "        self.b =  theano.shared(np.zeros(out_dim).astype(\"float64\"), name=\"bias\")\n",
    "        \n",
    "        self.params = [ self.W, self.b ]\n",
    "\n",
    "        self.set_pretraining()\n",
    "\n",
    "    def fprop(self, x):\n",
    "        ## WRITE ME Done\n",
    "        h = self.function(T.dot(x, self.W)+self.b)\n",
    "        self.h = h\n",
    "        return self.h\n",
    "\n",
    "    \n",
    "    def set_pretraining(self):\n",
    "        ae = Autoencoder(self.in_dim,self.out_dim,self.W,self.function)\n",
    "\n",
    "        x = T.matrix('x')\n",
    "        noise = T.matrix('noise')\n",
    "\n",
    "        cost,reconst_x = ae.reconst_error(x,noise)\n",
    "        params  = ae.params\n",
    "        gparams = T.grad(cost, params)\n",
    "        updates = sgd(params,gparams)\n",
    "\n",
    "        self.pretraining = theano.function([x,noise], [cost,reconst_x], updates=updates, allow_input_downcast=True)\n",
    "        \n",
    "        hidden = ae.encode(x)\n",
    "        self.encode_function = theano.function([x], hidden, allow_input_downcast=True)\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(mnist_x, mnist_y, test_size=0.2, random_state=42)\n",
    "\n",
    "activation = T.nnet.sigmoid #T.tanh\n",
    "\n",
    "layers = [## WRITE ME Done\n",
    "    Layer(784, 500, activation),\n",
    "    Layer(500, 500, activation),\n",
    "    Layer(500, 500, activation),\n",
    "    Layer(500, 10, T.nnet.softmax)\n",
    "]\n",
    "\n",
    "#Pre-training\n",
    "X = train_X\n",
    "for l, layer in enumerate(layers[:-1]):\n",
    "    corruption_level = 0.3 ## WRITE ME\n",
    "    batch_size = 100\n",
    "    nbatches = X.shape[0] // batch_size\n",
    "\n",
    "    for epoch in range(10):\n",
    "        ## WRITE ME\n",
    "        X = shuffle(X)\n",
    "        err_all=[]\n",
    "        for i in range(0,nbatches):\n",
    "            start = i * batch_size\n",
    "            end   = start + batch_size\n",
    "\n",
    "            noise = rng.binomial(size=X[start:end].shape, n=1, p=1-corruption_level)\n",
    "            err,reconst_x = layer.pretraining(X[start:end],noise)\n",
    "            err_all.append(err)\n",
    "\n",
    "        print \"Pre-training:: layer:%d, Epoch:%d, Error:%lf\" %(l,epoch, np.mean(err_all))\n",
    "    X = layer.encode_function(X)\n",
    "\n",
    "#Fine-tuning\n",
    "x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "params = []\n",
    "layer_out = None\n",
    "for i, layer in enumerate(layers):\n",
    "    params += layer.params\n",
    "    #print i, layer_out, layer.W\n",
    "    if i == 0:\n",
    "        layer_out = layer.fprop(x)\n",
    "    else:\n",
    "        layer_out = layer.fprop(layer_out)\n",
    "\n",
    "y = layers[-1].h\n",
    "cost = - T.mean((T.log(y))[T.arange(x.shape[0]), t])\n",
    "\n",
    "gparams = T.grad(cost, params)\n",
    "updates = sgd(params,gparams)\n",
    "\n",
    "train = theano.function([x,t], cost, updates=updates)\n",
    "valid  = theano.function([x,t],[cost, T.argmax(y, axis=1)])\n",
    "test  = theano.function([x],T.argmax(y, axis=1))\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_X.shape[0]//batch_size\n",
    "for epoch in range(50):\n",
    "    ## WRITE ME\n",
    "    train_X, train_y = shuffle(train_X, train_y)  # Shuffle Samples !!\n",
    "    for i in range(nbatches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            train(train_X[start:end], train_y[start:end])\n",
    "    valid_cost, pred = valid(valid_X, valid_y)\n",
    "\n",
    "    print \"EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f\"%(epoch+1, valid_cost, f1_score(valid_y, pred, average=\"macro\"))\n",
    "\n",
    "#pred_y = test(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
